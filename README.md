# GPT-2 Implementation from Scratch

A PyTorch implementation of GPT-2 with Flash Attention support. This implementation focuses on efficiency and readability while maintaining good performance.

## Features
- Flash Attention and traditional attention implementations
- Configurable architecture (embedding size, heads, layers, etc.)
- Checkpoint saving and loading
- Training progress tracking
- Memory efficient

## Requirements
- PyTorch
- safetensors
- numpy
